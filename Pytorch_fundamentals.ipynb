{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all documentation find it at https://pytorch.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.0+cu121\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intro Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Information\n",
    "- Anytime that you encode data into numbers when using pytorch it is of tensor type. \n",
    "- Typically when dealing with a scalar or vector the data is usually represented as a lower case letter, but for matrix and tensor it is upper case\n",
    "    - scalar(a), vector(y), MATRIX(Q),TENSOR(X) *Note this is just common practice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#most things works off tensors\n",
    "scalar = torch.tensor(7)\n",
    "scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a scalar has no dimensions. So we get back 0\n",
    "scalar.ndim "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To get the tensor back as a int\n",
    "scalar.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7, 7])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vectors\n",
    "vector = torch.tensor([7,7])\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vectors do have dimensions\n",
    "vector.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7,  8],\n",
       "        [ 9, 10]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MATRIX\n",
    "MATRIX = torch.tensor([[7,8],\n",
    "                       [9,10]])\n",
    "                       \n",
    "MATRIX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The number of dimensions is equal to the number of []\n",
    "MATRIX.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have to use print statement if there are more than one per box. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 9, 10])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tensor work the same way as an array\n",
    "print(MATRIX[0])\n",
    "MATRIX[1]\n",
    "\n",
    "# can also be printed as a tensor\n",
    "#MATRIX[0], MATRIX[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the shape is equal to the number of (row,column)\n",
    "MATRIX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode each matrix in it's own pair of []. This is so the computer can see that it has 1 bracket of [x,y]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1, 2, 3],\n",
       "         [4, 5, 6],\n",
       "         [7, 8, 9]]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TENSOR \n",
    "TENSOR = torch.tensor([[[1,2,3],\n",
    "                        [4,5,6],\n",
    "                        [7,8,9]]])\n",
    "\n",
    "TENSOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see it action. [1 matrix of [[3,3]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TENSOR.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 3])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TENSOR.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [7, 8, 9]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TENSOR[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random tensors\n",
    "\n",
    "Random tensors are used because they most neural networks start with a set of random tensors and then adjust those numbers in a way that \n",
    "the data can be more easily represented. \n",
    "\n",
    "How neural networks work\n",
    "\n",
    "Start with random numbers -> look at data -> update random numbers -> look at data -> update random numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5018, 0.2626, 0.9760, 0.2006],\n",
       "        [0.1033, 0.3656, 0.0740, 0.4688],\n",
       "        [0.2872, 0.1267, 0.4405, 0.0334]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a random tensor of size(3,4) (size/shape are the same thing)\n",
    "random_tensor = torch.rand(3,4)\n",
    "random_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tensor.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tensor.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*note that having size here does not matter this is just another way of creating a tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([224, 224, 3]), 3)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a random tensor with a similar shape to an image tensor\n",
    "random_image_size_tenor = torch.rand(size=(224,224,3))#height, width, color channels(R,G,B)\n",
    "random_image_size_tenor.shape,random_image_size_tenor.ndim #This prints in a tensor format\n",
    "#color channels can also be put first torch.rand(size=(3,224,224))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zeros and Ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros = torch.zeros(3,4)\n",
    "zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.],\n",
       "        [1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones = torch.ones(3,4)\n",
    "ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones.dtype\n",
    "#dtype = data type \n",
    "# the default data type is float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a range of tensors and tensors-like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  5, 10, 15, 20, 25, 30, 35, 40, 45])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#use torch.arange() torch.range is the same thing but an out of data way to do it. \n",
    "one_to_ten = torch.arange(0,10) #includes 0 but, excludes the 10\n",
    "print(one_to_ten)\n",
    "\n",
    "#step is something is goes up by\n",
    "skip_a_few = torch.arange(0,50,5)\n",
    "#skip_a_few = torch.arange(start=0,end=50,step=5) this is the same thing\n",
    "skip_a_few"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating tensors like creates a tensor but replaces the numbers with zeros \n",
    "ten_zeros = torch.zeros_like(input=one_to_ten)\n",
    "print(ten_zeros)\n",
    "\n",
    "# same thing but with ones. \n",
    "ten_ones = torch.ones_like(input=skip_a_few)\n",
    "ten_ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The tensor is default float 32 but if you need something with more precise then use float64 if less than use float 16\n",
    "\n",
    "Three of the biggest reasons for error are:\n",
    "1. Tensor not of the right data type\n",
    "2. Tensor not of the right shape (linear algebra and matrix theory)\n",
    "3. Tensor not on the right device\n",
    "    - This happens when one of tensors is running on a cpu but the other is on gpu/tpu/cuda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 6., 9.])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Float 32 tensor\n",
    "float_32_tensor = torch.tensor([3.0,6.0,9.0],\n",
    "                               dtype=None, #what data type is the tensor (float32 or float16)\n",
    "                               device=None, #Default this uses the cpu\n",
    "                               requires_grad=False) #This is for if you want pytorch to track the gradient \n",
    "float_32_tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3., 6., 9.], dtype=torch.float16)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# .type(torch.half) == .type(torch.float16). *best practice use float16\n",
    "float_16_tensor = float_32_tensor.type(torch.float16)\n",
    "float_16_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9., 36., 81.], dtype=torch.float64)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_16_tensor.type(torch.float64) *float_32_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some operations may cause an error when data types don't match but not always."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 9., 36., 81.])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float_16_tensor.type(torch.int32) *float_32_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting information from the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2623, 0.2642, 0.8172, 0.1928],\n",
       "        [0.7671, 0.5103, 0.1462, 0.9365],\n",
       "        [0.1445, 0.8028, 0.8509, 0.8200]])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_tensor = torch.rand(3,4)\n",
    "some_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "some_tensor.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.2623, 0.2642, 0.8172, 0.1928],\n",
      "        [0.7671, 0.5103, 0.1462, 0.9365],\n",
      "        [0.1445, 0.8028, 0.8509, 0.8200]])\n",
      "Datatype of tensor: torch.float32\n",
      "Shape of tensor: torch.Size([3, 4])\n",
      "Device of tensor: cpu\n"
     ]
    }
   ],
   "source": [
    "# these can help with checking if the tensor match. \n",
    "print(some_tensor)\n",
    "print(f\"Datatype of tensor: {some_tensor.dtype}\")\n",
    "print(f\"Shape of tensor: {some_tensor.shape}\")\n",
    "print(f\"Device of tensor: {some_tensor.device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4502, 0.3993, 0.4856],\n",
       "        [0.0008, 0.0459, 0.2773],\n",
       "        [0.5888, 0.3622, 0.6680]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_tensor = torch.rand(3,3)\n",
    "device_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to go.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if cuda(gpu) is available.\n",
    "#moves the device to cuda(gpu)\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Ready to go.\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"Failed to load\")\n",
    "    device = torch.device(\"cpu\")\n",
    "device_tensor = torch.device(device)\n",
    "device_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#moving bach to cpu\n",
    "device_tensor = torch.device(\"cpu\")\n",
    "device_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating Tensors(Tensor Operations)\n",
    "\n",
    "Tensor operations include:\n",
    "\n",
    "Addition\n",
    "\n",
    "Subtraction\n",
    "\n",
    "Multiplication(element-wise)\n",
    "\n",
    "Division\n",
    "\n",
    "Matrix multiplication\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding: tensor([11, 12, 13])\n",
      "Subtraction: tensor([-9, -8, -7])\n",
      "Multiply: tensor([2, 4, 6])\n",
      "Divide: tensor([0.1000, 0.2000, 0.3000])\n"
     ]
    }
   ],
   "source": [
    "tensor = torch.tensor([1,2,3])\n",
    "print(f\"Adding: {tensor + 10}\")\n",
    "print(f\"Subtraction: {tensor - 10}\")\n",
    "print(f\"Multiply: {tensor * 2}\")\n",
    "print(f\"Divide: {tensor / 10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 20, 30])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pytorch in-built functions\n",
    "torch.mul(tensor, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix multiplication\n",
    "1. element wise (mulitply by a scalar)\n",
    "2. Dot product\n",
    "\n",
    "**(rows,cols)*\n",
    "\n",
    "This is the the same as from linear algebra and matrix theroy. \n",
    "# Rules\n",
    "1. the inner dimensions must match. for example:\n",
    "    \n",
    "    [1, 2] @ [3, 4] will not compute \n",
    "    \n",
    "    [5, 6] @ [6, 8] will compute\n",
    "2. The resulting matrix will have the shape of the outer dimensions for example:\n",
    "    \n",
    "    (3,4) @ (4,3) The new shape will be (3,3)\n",
    "    \n",
    "    (3,5) @ (5,1) the new shape will be (3,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.9491],\n",
       "        [1.4992],\n",
       "        [2.1190]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#example\n",
    "torch.matmul(torch.rand(3,5), torch.rand(5,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4, 9])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Element-wise multiplication\n",
    "tensor * tensor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])  *  tensor([1, 2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dot product\n",
    "print(tensor,\" * \",  tensor)\n",
    "torch.matmul(tensor, tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dot product does (1 * 1)+(2 * 2)+(3 * 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timing how long each of the codes take. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(14)\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 544 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "value = 0\n",
    "for i in range(len(tensor)):\n",
    "    value += tensor[i] * tensor[i]\n",
    "print(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time \n",
    "torch.matmul(tensor,tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#also another way to do matrix multiplication dot product\n",
    "#this takes the same amount of time as above\n",
    "tensor @ tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 0 ns\n",
      "Wall time: 0 ns\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(14)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "tensor @ tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matrix multiplication shape errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This will give an error because shapes do not match \n",
    "tensor_A = torch.tensor([[1,2],\n",
    "                        [3,4],\n",
    "                        [5,6]])\n",
    "tensor_B = torch.tensor([[7,10],\n",
    "                         [8,11],\n",
    "                         [9,12]])\n",
    "#tensor_A @ tensor_B\n",
    "#torch.mm(tensor_A, tensor_B) #mm = matmul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiplying matrix's with shapes that do not match. \n",
    "\n",
    "By using a transpose you can manipulate the shape of the matrix. This will switch the axis of the given tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 3, 5],\n",
       "         [2, 4, 6]]),\n",
       " tensor([[1, 2],\n",
       "         [3, 4],\n",
       "         [5, 6]]))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#.T is the transpose of the original matrix\n",
    "tensor_A.T, tensor_A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]), torch.Size([3, 2]))"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can see that the transpose will change the dimensions of the matrix by checking the shape\n",
    "tensor_A.T.shape, tensor_A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 27,  30,  33],\n",
       "        [ 61,  68,  75],\n",
       "        [ 95, 106, 117]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_A @ tensor_B.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This will give the new shape of: torch.Size([3, 3])\n"
     ]
    }
   ],
   "source": [
    "new_tensor = torch.mm(tensor_A, tensor_B.T)\n",
    "print(f\"This will give the new shape of: {new_tensor.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensors, min, max, mean, sum. (tensor aggregation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  6, 12, 18, 24, 30, 36, 42, 48, 54, 60, 66, 72, 78, 84, 90, 96])"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "x = torch.arange(0,100,random.randint(5,10))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum value in the array:  tensor(96)\n",
      "Minimum value in the array:  tensor(0)\n",
      "Sum of all elements in the array:  tensor(816)\n"
     ]
    }
   ],
   "source": [
    "#Finding the max min and sum\n",
    "max = x.max()\n",
    "min = x.min()\n",
    "sum = x.sum()\n",
    "\n",
    "#another way torch.min(x)... \n",
    "\n",
    "print(\"Maximum value in the array: \", max)\n",
    "print(\"Minimum value in the array: \", min)\n",
    "print(\"Sum of all elements in the array: \", sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mean value in the tensor is: 48.0\n",
      "Second way to get the mean value in the tensor is: 48.0\n"
     ]
    }
   ],
   "source": [
    "# For mean process is slightly different, you have to do this because the datatype's do not match. \n",
    "# when computing it normally it will give you a long but wants a float so you get an error.\n",
    "mean = torch.mean(x.type(torch.float))\n",
    "mean2 = x.type(torch.float32).mean()\n",
    "print(f\"The mean value in the tensor is: {mean}\")\n",
    "print(f\"Second way to get the mean value in the tensor is: {mean2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([17])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor(0), tensor(16))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#finding the position of where the max/min values are \n",
    "print(x.shape)\n",
    "x.argmin(), x.argmax(),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(16)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "position_max = torch.argmax(x)\n",
    "position_max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reshaping, Stacking, Squeezing, Unsqueezing, and Permuting tensor\n",
    "\n",
    "* Reshaping - reshapes the tensor in your desired way\n",
    "\n",
    "* View - Returns a view of an input tensor of certain shape but keeps the same memory as the original tensor. \n",
    "\n",
    "* Stacking - combines multiple tensor on top of each other\n",
    "    - .vstack (vertically stacking)\n",
    "    - .hstack (horizontally stacking)\n",
    "\n",
    "* Squeezing - removes single-dimensional entries from the shape of a tensor.\n",
    "\n",
    "* Unsqueezing - adds a new dimension of size 1 to the tensor at the specified position.\n",
    "\n",
    "* Permute - return a view of the input with dimensions permuted(swapped) in a certain way\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.]), torch.Size([9]))"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(1.,10.)\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3., 4., 5., 6., 7., 8., 9.]])\n",
      "Own tensors: torch.Size([9, 1])\n",
      "Reshaped: torch.Size([1, 9])\n",
      "Normal: torch.Size([9])\n",
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [4.],\n",
      "        [5.],\n",
      "        [6.],\n",
      "        [7.],\n",
      "        [8.],\n",
      "        [9.]])\n"
     ]
    }
   ],
   "source": [
    "# Adding a dimension\n",
    "# x_reshaped = x.reshape(1,7) this won't work because you're trying to push in 9 dimensions into 7 places\n",
    "x_reshaped = x.reshape(1,9) # pushing 9 inputs into 1 dimension\n",
    "x_own_tensors = x.reshape(9,1) # This will put 9 inputs into a their own tensor's \n",
    "print(x_reshaped)\n",
    "\n",
    "print(f\"Own tensors: {x_own_tensors.shape}\")\n",
    "print(f\"Reshaped: {x_reshaped.shape}\")\n",
    "print(f\"Normal: {x.shape}\")\n",
    "print(x_own_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2., 3.],\n",
       "         [4., 5., 6.],\n",
       "         [7., 8., 9.]]),\n",
       " torch.Size([3, 3]))"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# As long as the .reshape(x,y) multiply equal to the shape/size of the original it can be reshaped as such\n",
    "x_test_reshape = x.reshape(3,3)\n",
    "x_test_reshape, x_test_reshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2., 3., 4., 5., 6., 7., 8., 9.]]), torch.Size([1, 9]))"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view will show the same thing as reshape, BUT remember that view keeps the same memory as the original. \n",
    "# WHATEVER CHANGES ARE MADE TO THE OBJECT IN VIEW WILL CHANGE THE ORIGINAL \n",
    "z = x.view(1,9)\n",
    "z, z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 5., 3., 4., 5., 6., 7., 8., 9.]]),\n",
       " tensor([1., 5., 3., 4., 5., 6., 7., 8., 9.]))"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for example if i change the second number for z, then the the second number in x will change as well. \n",
    "z[:,1] = 5\n",
    "z,x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vstack will stack the following on top of one another in their own tensors.\n",
    "\n",
    "hstack will merge the following into one tensor\n",
    "\n",
    "(v/h)stack([x,x,x], dim=0) 0 is the same as vstack and 1 will take each number and repeat as a tensor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 5., 3., 4., 5., 6., 7., 8., 9.],\n",
       "        [1., 5., 3., 4., 5., 6., 7., 8., 9.],\n",
       "        [1., 5., 3., 4., 5., 6., 7., 8., 9.],\n",
       "        [1., 5., 3., 4., 5., 6., 7., 8., 9.]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stacking tensors\n",
    "\n",
    "x_stacked = torch.vstack([x,x,x,x])\n",
    "x_stacked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "with dimension: tensor([[1., 5., 3., 4., 5., 6., 7., 8., 9.]])\n",
      "Removed dimension: tensor([1., 5., 3., 4., 5., 6., 7., 8., 9.])\n"
     ]
    }
   ],
   "source": [
    "# drops all dimension that have a size of 1. \n",
    "x_one_dim = x.reshape(1,9)\n",
    "x_squeezed = torch.squeeze(x_one_dim)\n",
    "\n",
    "print(f\"with dimension: {x_one_dim}\")\n",
    "print(f\"Removed dimension: {x_squeezed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.],\n",
       "         [5.],\n",
       "         [3.],\n",
       "         [4.],\n",
       "         [5.],\n",
       "         [6.],\n",
       "         [7.],\n",
       "         [8.],\n",
       "         [9.]]),\n",
       " torch.Size([9, 1]),\n",
       " tensor([1., 5., 3., 4., 5., 6., 7., 8., 9.]),\n",
       " torch.Size([9]))"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# adds in dimensions at the chosen position\n",
    "# must be in range of [-2, 1]\n",
    "# 1/-1 puts all in their own tensors\n",
    "# 0/-2 puts all in one tensor\n",
    "x_unsqueezed = torch.unsqueeze(x,1)\n",
    "x_unsqueezed, x_unsqueezed.shape, x, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " original shpae is: torch.Size([3, 2, 4])\n",
      " After permute: torch.Size([2, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "# to permute a tensor is the same as to swap the dimensions. \n",
    "x_permute = torch.rand(3,2,4)\n",
    "print(f\" original shpae is: {x_permute.shape}\")\n",
    "\n",
    "# after permute\n",
    "print(f\" After permute: {torch.permute(x_permute,(1,2,0)).shape}\")\n",
    "#another way to do it. use this way\n",
    "permuted = x_permute.permute((1,2,0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing\n",
    "\n",
    "Indexing with pytorch is the same as indexing in numpy\n",
    "\n",
    "first number is all elements in that dimension\n",
    "    \n",
    "second number selects rows\n",
    "\n",
    "third number selects the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1, 2, 3],\n",
       "          [4, 5, 6],\n",
       "          [7, 8, 9]]]),\n",
       " torch.Size([1, 3, 3]))"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(1,10,).reshape(1,3,3)\n",
    "x,x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "tensor([1, 2, 3])\n",
      "tensor(1)\n",
      "tensor(9)\n"
     ]
    }
   ],
   "source": [
    "# Index on tensor\n",
    "print(x[0]) # first zero gets inside the first set of brackets\n",
    "print(x[0,0]) # second zero give back the 0 dimensions of the x[0] tensor \n",
    "print(x[0,0,0]) # third zero give back the 0 dimensions of the x[0,0]\n",
    "\n",
    "# to get back a certain value like 9\n",
    "print(x[0,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The : gets all the the targets dimension\n",
    "x[:,0] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 5, 8]])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:,:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3],\n",
       "        [4, 5, 6],\n",
       "        [7, 8, 9]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#all values of the 0 dimension but only the 1 index value of 1st and 2nd dimension\n",
    "x[:,1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index 0 of 0th and 1st dimension and all values of the 2nd dimension\n",
    "x[0, 0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4, 7])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0,:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3, 6, 9]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# return 3,6,9\n",
    "x[:,:,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch tensors and numpy\n",
    "\n",
    "Numpy is a popular scientific python numerical library. \n",
    "\n",
    "And because of this, pytorch hasa functionality to interact with it. When transferring the data it will be transferred in the same data type as it is currently stored in.\n",
    "\n",
    "* Data in numpy, want input torch tensor -> torch.from_numpy(ndarray)\n",
    "* Pyorch tensor -> numpy -> torch.tensor.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Numpy is not available",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[144], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# numpy array to tensor\u001b[39;00m\n\u001b[0;32m      2\u001b[0m array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m1.0\u001b[39m,\u001b[38;5;241m8.0\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#.type(torch.float32)      changing the datatype\u001b[39;00m\n\u001b[0;32m      4\u001b[0m array, tensor, tensor\u001b[38;5;241m.\u001b[39mdtype\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Numpy is not available"
     ]
    }
   ],
   "source": [
    "# numpy array to tensor\n",
    "array = np.arange(1.0,8.0)\n",
    "tensor = torch.from_numpy(array) #.type(torch.float32)      changing the datatype\n",
    "array, tensor, tensor.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* numpy uses a default of float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 1., 1., 1., 1., 1., 1.]),\n",
       " array([1., 1., 1., 1., 1., 1., 1.], dtype=float32),\n",
       " dtype('float32'))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# form tenosr to numpy\n",
    "tensor = torch.ones(7)\n",
    "numpy_tensor = tensor.numpy()\n",
    "tensor, numpy_tensor, numpy_tensor.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two do not share the same memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reproducing (taking out randomness)\n",
    "\n",
    "To reduce the amount of randomness in neural networks and pytorch comes the concept of a random seed. essentially what the radom seed does is \"flavor\" the randomness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1089, 0.3158, 0.4755, 0.3832],\n",
      "        [0.2010, 0.6830, 0.3285, 0.3106],\n",
      "        [0.8588, 0.1114, 0.2163, 0.6617]])\n",
      "tensor([[0.7375, 0.0613, 0.0378, 0.9382],\n",
      "        [0.2896, 0.6513, 0.3586, 0.7292],\n",
      "        [0.0442, 0.3551, 0.0371, 0.7083]])\n",
      "tensor([[False, False, False, False],\n",
      "        [False, False, False, False],\n",
      "        [False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "# random tensors\n",
    "random_tensor_A = torch.rand(3,4)\n",
    "random_tensor_B = torch.rand(3,4)\n",
    "\n",
    "print(random_tensor_A) \n",
    "print(random_tensor_B)\n",
    "print(random_tensor_A == random_tensor_B)\n",
    "#they will almost never be equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0918, 0.4794, 0.8106, 0.0151],\n",
      "        [0.0153, 0.6036, 0.2318, 0.8633],\n",
      "        [0.9859, 0.1975, 0.0830, 0.4253]])\n",
      "tensor([[0.0918, 0.4794, 0.8106, 0.0151],\n",
      "        [0.0153, 0.6036, 0.2318, 0.8633],\n",
      "        [0.9859, 0.1975, 0.0830, 0.4253]])\n",
      "tensor([[True, True, True, True],\n",
      "        [True, True, True, True],\n",
      "        [True, True, True, True]])\n"
     ]
    }
   ],
   "source": [
    "# How to make it reproducible\n",
    "\n",
    "# first set a random seed\n",
    "RANDOM_SEED = 13\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random_tensor_C = torch.rand(3,4)\n",
    "\n",
    "# YOU MUST RESET THE RANDOM_SEED EACH AND EVERY TIME YOU USE A NEW RANDOM METHOD\n",
    "\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "random_tensor_D = torch.rand(3,4)\n",
    "\n",
    "print(random_tensor_C)\n",
    "print(random_tensor_D)\n",
    "print(random_tensor_C == random_tensor_D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running tensors with the gpu\n",
    "\n",
    "This happens with the help of nvidia hardware, CUDA, and pytorch.\n",
    "\n",
    "If you don't want to use your own gpu you can use:\n",
    " 1. google colab\n",
    " 2. OSC or juptyr notebook\n",
    " 3. AWS or Azure notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting up device agnostic code\n",
    "# if gpu is available then use it\n",
    "device = \"cuda\" if torch.cuda.is_available else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of device that can be used\n",
    "# can be useful is running multiple models and run them on different ones. \n",
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3], device='cuda:0')"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# putting tensors or gpu for faster performance\n",
    "tensor = torch.tensor([1,2,3])\n",
    "tensor, tensor.device\n",
    "\n",
    "# tensors default to cpu but can be moved to gpu\n",
    "tensor_on_gpu = tensor.to(device)\n",
    "tensor_on_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all things work with the gpu. For example numpy does not work with gpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# moves back to cpu\n",
    "tensor_on_gpu.cpu().numpy()\n",
    "tensor_on_gpu.cpu().device"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
